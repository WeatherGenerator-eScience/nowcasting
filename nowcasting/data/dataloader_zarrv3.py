import zarr
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import lightning as L

from omegaconf import OmegaConf

from typing import Optional
import warnings

# Import custom transformation and sample generation utilities
from nowcasting.data.utils.transforms import set_transforms
from nowcasting.data.utils.generate_samples import get_sample_dfs

def scale_slice(slc: slice, factor: int) -> slice:
    """
    Scales the start, stop, and step attributes of a slice object by a given factor.

    This utility function is used to adjust spatial or temporal slices when
    accessing data from Zarr groups that might have different resolutions
    (e.g., downscaled images or different temporal intervals).

    Parameters:
        slc : slice
            The input slice object (e.g., `slice(0, 100, 2)`).
        factor : int
            The scaling factor. Start, stop, and step values will be divided by this factor.

    Returns:
        slice
            A new slice object with its attributes scaled. If any attribute is `None`,
            it remains `None`.
    """
    return slice(
        slc.start // factor if slc.start is not None else None,
        slc.stop // factor if slc.stop is not None else None,
        slc.step // factor if slc.step is not None else None
    )

class NowcastingDataset(Dataset):
    """
    A PyTorch Dataset for loading and processing nowcasting data from a ZarrV3 store.

    This dataset handles the retrieval of context (input) and forecast (target)
    sequences, applying various transformations, and supporting flexible input
    structures (e.g., for multi-modal models). It operates on a DataFrame of
    sample indices generated by `get_sample_dfs`.

    Parameters:
        df : pd.DataFrame
            A DataFrame where each row represents a data sample. It must contain
            't_idx', 'h_idx', and 'w_idx' columns, which are the base time,
            height block, and width block indices for the sample.
        root : zarr.Group
            The opened Zarr group object representing the root of the dataset.
            This is used to access the actual data arrays and their attributes.
        info : dict
            A dictionary containing configuration details for data loading and
            transformation. Expected keys include:
            - 'in_vars' (list): Specifies input variables. Can be a flat list
              (e.g., `["radar/rtcor"]`) or a nested list (e.g., `[["radar/rtcor"], ["sat/IR"]]`)
              to group variables for multi-input models.
            - 'out_vars' (list): Specifies output (target) variables.
            - 'transforms' (dict): A dictionary mapping Zarr group names or
              full variable paths to callable transformation functions (e.g.,
              `{"radar": <transform_func>}`). These functions are typically
              set up by `set_transforms`.
            - 'latlon' (bool, optional): If True, includes latitude and longitude
              maps as additional channels/groups in the context data.
        size : tuple[int, int], default (8, 8)
            The spatial dimensions of the extracted data patch, in units of `block_size`.
            For example, `(8, 8)` means 8 blocks in height and 8 blocks in width.
        block_size : int, default 32
            The size (in pixels/grid cells) of a single spatial block. The total
            spatial dimension of a sample will be `size[0] * block_size` by
            `size[1] * block_size`.
        context_len : int, default 4
            The number of time steps to include in the context (input) sequence.
        forecast_len : int, default 18
            The number of time steps to include in the forecast (target) sequence.
        include_timestamps : bool, default False
            If True, a 1D NumPy array of relative timestamps (from `-(context_len-1)` to `0`)
            is included with the context data. For nested `in_vars`, it's added
            to each group; otherwise, it's a separate tensor.
    """
    def __init__(
        self,
        df: pd.DataFrame,
        root: zarr.Group,
        info: dict,
        size: tuple[int, int] = (8, 8),
        block_size: int = 32,
        context_len: int = 4,
        forecast_len: int = 18,
        include_timestamps: bool = False
    ):
        self.df = df
        self.root = root
        self.info = info
        self.size = size
        self.block_size = block_size
        self.context_len = context_len
        self.forecast_len = forecast_len
        self.include_timestamps = include_timestamps

        # Determine output format based on in_vars structure
        in_vars = self.info.get('in_vars', [])
        self.nested_output = len(in_vars) > 0 and isinstance(in_vars[0], (list, tuple))

        # Identify the main variable (usually the radar data) to infer overall
        # dataset properties like spatial shape for AWS cropping.
        self.main_var = in_vars[0][0] if self.nested_output else (in_vars[0] if in_vars else None)
        self.main_var_shape = self.root[self.main_var].shape if self.main_var else None

        # Load static latitude and longitude data if requested.
        if self.info.get('latlon', False):
            main_group = self.main_var.split('/')[0] if self.main_var else None
            if main_group:
                self.lat = self.root[main_group]['lat'][:].astype(np.float32)
                self.lon = self.root[main_group]['lon'][:].astype(np.float32)
            else:
                warnings.warn("latlon requested but no main variable available")

    def __len__(self):
        return len(self.df)
    
    def _load_data(self, var: str, time_start: int, time_end: int, img_slice: tuple[slice, slice]) -> np.ndarray:
        """
        Loads and processes data for a single variable within a specified time and spatial range.

        This internal helper handles variable-specific loading logic, including:
        - Adjusting temporal and spatial slicing based on Zarr group attributes
          (`interval_minutes`, `downscale_factor`).
        - Special handling for 'aws' data (loading values and locations).
        - Applying configured transformations.
        - Handling temporal downsampling by ensuring correct indexing for the output.

        Parameters:
            var : str
                The Zarr path to the variable (e.g., "radar/rtcor").
            time_start : int
                The starting time index (in 5-minute global steps) for the data segment.
            time_end : int
                The ending time index (exclusive, in 5-minute global steps) for the data segment.
            img_slice : tuple[slice, slice]
                A tuple of two slice objects `(slice(h0, h1), slice(w0, w1))` defining
                the spatial region to load from the main variable's grid.

        Returns:
            Union[np.ndarray, tuple[np.ndarray, np.ndarray]]
                The loaded and transformed data. For 'aws' data before interpolation,
                it returns a tuple `(values, locations)`. Otherwise, it returns
                a NumPy array.
        """
        # Extract the top-level Zarr group name (e.g., 'radar', 'harmonie')
        group = var.split('/')[0]

        # Get the temporal interval of the current group's data relative to the 5-minute global interval.
        t_scale = self.root[group].attrs['interval_minutes'] // 5

        # Determine spatial slicing based on the group type and its downscale factor.
        if group == 'aws':
            # AWS data is typically sparse station data, not a grid, so spatial slicing is not applied directly.
            scaled_img_slice = (slice(None),) # Load all stations
        else:
            # For gridded data, scale the image slice according to the group's downscale factor.
            img_scale = self.root[group].attrs['downscale_factor']
            scaled_img_slice = tuple(scale_slice(s, img_scale) for s in img_slice)

        # Handle temporal indexing, especially for 'harmonie' (forecast model data).
        # Harmonie data might have a forecast range (e.g., forecasts starting at certain lead times).
        if group == 'harmonie':
            forecast_start = self.root[group].attrs['forecast_range'][0] # The first few forecasts are left out, as they contain model spin-up
            # Adjust time indices based on forecast start and group's temporal scale.
            t0_temp = time_start - forecast_start * 12 # 12 steps per hour for 5min interval
            t1_temp = time_end - forecast_start * 12
            forecast_idx = t0_temp // t_scale
            time_range = (np.arange(t0_temp, t1_temp) - (forecast_idx * t_scale)) // 12
            forecast_idx = max(forecast_idx, 0)
            time_slice = (forecast_idx, slice(time_range[0], time_range[-1] + 1))
        else:
            # For other groups, simply scale the global time indices to the group's temporal resolution.
            time_range = np.arange(time_start, time_end) // t_scale
            time_slice = (slice(time_range[0], time_range[-1] + 1),)
        
        # Load data
        data_slice = time_slice + scaled_img_slice
        frame = self.root[var][data_slice].astype(np.float32)

        # If the data is from 'aws', also load its corresponding location data.
        if group == 'aws':
            locs = self.root[var + '_loc'][:]
            frame = (frame, locs)

        # Apply transformations. Transformations can be defined for the entire group
        # or for a specific variable. Group transforms take precedence.
        for transform_key in [group, var]:
            if transform_key in self.info.get('transforms', {}):
                frame = self.info['transforms'][transform_key](frame)

        # Special handling for AWS data after transformation: if it has been
        # interpolated into an image, crop it to match the main variable's spatial shape.
        if group == 'aws' and frame.shape[-2:] == self.main_var_shape[-2:]:
            # Crop the aws image in case other images are also cropped
            data_slice = (slice(None),) + img_slice
            frame = frame[data_slice]

        # Handle temporal downsampling/upsampling:
        # If the group's temporal resolution is different from the 5-minute global step,
        # `time_range` might have duplicate indices. `np.unique(..., return_inverse=True)`
        # ensures that data is correctly indexed to match the `context_len` or `forecast_len`.
        _, inverse_indices = np.unique(time_range, return_inverse=True)
        return frame[inverse_indices]
    
    def _load_group_data(self, group_vars: list[str], time_start: int, time_end: int, spatial_slice: tuple[slice, slice]) -> np.ndarray:
        """
        Loads and stacks all variables within a logical group along the channel dimension.

        This is used when `in_vars` is a nested list, allowing multiple variables
        (e.g., different satellite bands) to be treated as a single input group
        to a model branch.

        Parameters:
            group_vars : list[str]
                A list of Zarr paths to variables belonging to the same logical group.
            time_start : int
                The starting time index (global 5-minute step).
            time_end : int
                The ending time index (exclusive, global 5-minute step).
            spatial_slice : tuple[slice, slice]
                The spatial slice to apply to all variables in the group.

        Returns:
            np.ndarray
                A NumPy array with all variables in the group concatenated along
                a new channel dimension (shape: C, T, H, W).
        """
        group_data_list = []
        for var in group_vars:
            data = self._load_data(var, time_start, time_end, spatial_slice)
            # Ensure the loaded data has a channel dimension (C, T, H, W).
            # If it's (T, H, W), add a new dimension at axis 0.
            if data.ndim == 3:
                data = np.expand_dims(data, axis=0) # -> (1, T, H, W)
            group_data_list.append(data)
        # Concatenate all variables in the group along the channel dimension.
        return np.concatenate(group_data_list, axis=0) # Resulting shape: (C_total, T, H, W)
    
    def _create_latlon_data(self, spatial_slice: tuple[slice, slice], time_len: int) -> np.ndarray:
        """
        Creates a NumPy array containing latitude and longitude maps for the given
        spatial slice, broadcast across the time dimension.

        This static geographical information can be useful as additional input
        channels for a model.

        Parameters:
            spatial_slice : tuple[slice, slice]
                The spatial slice to extract from the global latitude and longitude maps.
            time_len : int
                The desired length of the time dimension (e.g., `context_len` or `forecast_len`).

        Returns:
            np.ndarray
                A NumPy array of shape `(2, T, H, W)` where the first channel is
                latitude and the second is longitude, for the specified time length
                and spatial region.
        """
        if self.lat is None or self.lon is None:
            # Return an empty array or handle error if lat/lon data wasn't loaded
            warnings.warn("Latitude/Longitude data not available for creation.")
            return np.empty((2, time_len, 0, 0), dtype=np.float32) # Return empty array with correct dimensions

        # Crop the global lat/lon maps to the current sample's spatial extent.
        lat_cropped = self.lat[spatial_slice] # (H, W)
        lon_cropped = self.lon[spatial_slice] # (H, W)
        
        # Broadcast the 2D lat/lon maps to match the (T, H, W) shape.
        lat_broadcast = np.broadcast_to(lat_cropped, (time_len, *lat_cropped.shape))
        lon_broadcast = np.broadcast_to(lon_cropped, (time_len, *lon_cropped.shape))
        
        # Stack them along a new channel dimension to get (2, T, H, W).
        return np.stack([lat_broadcast, lon_broadcast], axis=0)

    def __getitem__(self, idx: int):
        """
        Retrieves a single data sample (context and/or future) from the dataset.

        This method is called by PyTorch's DataLoader to fetch individual samples.
        It orchestrates the loading of data based on the sample's indices,
        applies transformations, and formats the output according to the
        `in_vars` and `out_vars` configurations.

        Parameters:
            idx : int
                The integer index of the sample to retrieve from the `self.df`.

        Returns:
            Union[Any, tuple[Any, Any]]
                The data sample.
                - If both `in_vars` and `out_vars` are defined, returns `(context, future)`.
                - If only `in_vars` is defined, returns `context`.
                - If only `out_vars` is defined, returns `future`.
                The `context` can be a single `np.ndarray` or a `list` of `np.ndarray`
                (or `list` of `[np.ndarray, np.ndarray]` if `include_timestamps` is True),
                depending on `self.nested_output`. `future` is always a single `np.ndarray`.

        Raises:
            ValueError: If both context and future data are empty (no `in_vars` or `out_vars` defined).
        """

        # Extract sample coordinates
        t0, h0, w0 = self.df.loc[idx, ['t_idx', 'h_idx', 'w_idx']].astype(int)

        # Calculate temporal ranges
        t1 = t0 + self.context_len
        t2 = t1 + self.forecast_len

        # Calculate spatial slice
        h0 = h0 * self.block_size
        w0 = w0 * self.block_size
        h1 = h0 + self.size[0]*self.block_size
        w1 = w0 + self.size[1]*self.block_size
        spatial_slice = (slice(h0,h1), slice(w0, w1))

        # Generate timestamps if needed
        if self.include_timestamps:
            # Timestamps range from -(context_len - 1) to 0 (inclusive).
            context_timesteps = np.arange(-self.context_len + 1, 1, dtype=np.float32)

        context = None
        in_vars = self.info.get('in_vars', [])
        if in_vars:
            if self.nested_output:
                # Nested list format
                context = []
                for group in in_vars:
                    group_data = self._load_group_data(group, t0, t1, spatial_slice)
                    
                    # Create group element [data, (optional) timestamps]
                    element = [group_data]
                    if self.include_timestamps:
                        element.append(context_timesteps.copy())
                    context.append(element)
                
                # Add lat/lon as separate group
                if self.info.get('latlon', False) and hasattr(self, 'lat'):
                    latlon_data = self._create_latlon_data(spatial_slice, self.context_len)
                    element = [latlon_data]
                    if self.include_timestamps:
                        element.append(context_timesteps.copy())
                    context.append(element)
            else:
                # Traditional stacked format
                context_data = []
                for var in in_vars:
                    data = self._load_data(var, t0, t1, spatial_slice)
                    if data.ndim == 3:  # Add channel dim if missing
                        data = np.expand_dims(data, axis=0)
                    context_data.append(data)
                
                context = np.concatenate(context_data, axis=0)
                
                # Add lat/lon if requested
                if self.info.get('latlon', False) and hasattr(self, 'lat'):
                    latlon_data = self._create_latlon_data(spatial_slice, self.context_len)
                    context = np.concatenate([context, latlon_data], axis=0)

        # Load future data (always stacked)
        future = None
        out_vars = self.info.get('out_vars', [])
        if out_vars:
            future_data = []
            for var in out_vars:
                data = self._load_data(var, t1, t2, spatial_slice)
                if data.ndim == 3:  # Add channel dim if missing
                    data = np.expand_dims(data, axis=0)
                future_data.append(data)
            future = np.concatenate(future_data, axis=0)

        # Return based on available data
        if context is not None and future is not None:
            return context, future
        elif context is not None:
            return context
        elif future is not None:
            return future
        else:
            raise ValueError("Both context and future data are empty")

class NowcastingDataModule(L.LightningDataModule):
    """
    A PyTorch Lightning DataModule for nowcasting datasets.

    This DataModule encapsulates all data-related logic, including:
    - Loading data from a Zarr store.
    - Defining dataset splits (train, validation, test) based on time rules.
    - Applying global time masks (e.g., for missing data or clutter).
    - Setting up data transformations.
    - Generating data samples (patches) and optionally applying weighted sampling
      for the training set.
    - Providing PyTorch DataLoaders for each split.

    It integrates with `NowcastingDataset` for sample loading and `get_sample_dfs`
    and `set_transforms` from the `nowcasting.data.utils` module.

    Parameters:
        path : str
            The file path to the root Zarr store containing the meteorological data.
        var_info : dict
            A dictionary defining variables to load, their transformations, and
            which variable to use for sample weighting. See `NowcastingDataset`
            `info` parameter for more details.
        split_info : dict
            A dictionary defining how to split the dataset temporally, and
            which masks to apply. Expected keys:
            - 'split_rules' (dict): Rules for assigning timestamps to 'train',
              'val', 'test' splits. Each key corresponds to a split name (e.g.,
              "train", "val", "test"), and its value is a dictionary specifying
              the temporal criteria for that split.
              
              Example: `{"test": {"year": [2023]}, "val": {"month": [6, 11]}, "train": {}}`
              
              * **Temporal Attributes:** Keys within a split's rule (e.g., "year", "month", "day",
                  "hour", "day_of_week") correspond to attributes of a `pandas.Timestamp`.
              * **List of Values:** The values for these attributes are lists of integers. A timestamp
                  matches the rule if its corresponding temporal attribute is present in the list.
              * **AND Logic:** If multiple temporal attributes are specified within one rule (e.g.,
                  `{"year": [2023], "month": [1]}`), a timestamp must satisfy *all* conditions.
              * **OR Logic:** If a list contains multiple values (e.g., `{"month": [6, 11]}`), a
                  timestamp matches if its month is 6 *or* 11.
              * **Prioritization:** Rules are typically applied in a defined order (e.g., 'test' and
                  'val' rules might be applied first, with a default empty `{}` rule for 'train'
                  catching all remaining timestamps). This ensures clean separation and avoids
                  data leakage.

            - 'apply_missing_masks' (list[str], optional): List of Zarr group
              names whose 'time_mask' attributes should be used to filter invalid
              timestamps. Where True means that a time index in pressent and False that it is missing.
            - 'clutter_threshold' (float, optional): A threshold for radar 'clutter_score'. 
              A pixel is marked as clutter if its gradient exceeds 30 mm/h.  If the number of 
              clutter pixels in a sample exceeds the 'clutter_threshold', the sample is masked.
              
        sample_info : dict, optional
            Configuration for sample generation and weighted sampling. Expected keys:
            - 'threshold' (float, optional): Filters samples based on their 'weight'
              from `sample_var`.
            - 'methods' (dict): Configuration for `get_sample_dfs` to determine
              how sample weights are aggregated (e.g., `{'train': {'agg': 'max_pool'}}`).
            - 'bins' (dict, optional): Configuration for `WeightedRandomSampler`
              for the training set, including `n_bins`, `first_bin`, `last_bin`,
              `slope`, `scaler`, `n_samples`.

        context_len : int, default 4
            Number of time steps for the input sequence.
        forecast_len : int, default 18
            Number of time steps for the output sequence.
        include_timestamps : bool, default False
            If True, relative timestamps are included in the context data.
        img_size : tuple[int, int], default (8, 8)
            The spatial dimensions of the extracted image patch in `block_size` units.
        stride : tuple[int, int, int], default (1, 1, 1)
            The stride (temporal, height, width) for generating samples.
        batch_size : int, default 16
            The batch size for DataLoaders.
        num_workers : int, default 8
            Number of subprocesses to use for data loading. Set to 0 for single-process.
    """
    def __init__(
        self,
        path: str,
        var_info: dict[str, object],
        split_info: dict[str, object],
        sample_info: dict[str, object] = {},
        context_len: int = 4,
        forecast_len: int = 18,
        include_timestamps: bool = False,
        img_size: tuple[int, int] = (8, 8),
        stride: tuple[int, int, int] = (1, 1, 1),
        batch_size: int = 16,
        num_workers: int = 8,
    ):

        super().__init__()
        self.path = path
        self.var_info = var_info
        self.split_info = split_info
        self.context_len = context_len
        self.forecast_len = forecast_len
        self.include_timestamps = include_timestamps
        self.img_size = img_size
        self.stride = stride
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.sample_info = sample_info
        self.sampler = None # Sampler for training data

        # Resolve standard python dicts to OmegaConf objects if they are passed as such.
        # This ensures consistent dictionary access.
        if not isinstance(self.var_info, dict):
            self.var_info = OmegaConf.to_container(self.var_info, resolve=True)
        if not isinstance(self.split_info, dict):
            self.split_info = OmegaConf.to_container(self.split_info, resolve=True)
        if not isinstance(self.sample_info, dict):
            self.sample_info = OmegaConf.to_container(self.sample_info, resolve=True)

    def setup(self, stage: Optional[str] = None):
        """
        Prepares the datasets for training, validation, and testing.

        This method is called by PyTorch Lightning at the beginning of training,
        validation, or testing. It performs the following steps:
        1. Opens the Zarr data store.
        2. Initializes data transformations.
        3. Gathers temporal metadata.
        4. Creates a global time mask based on data availability and clutter.
        5. Generates data sample DataFrames for each split (train, val, test).
        6. Instantiates `NowcastingDataset` for each split.
        7. Optionally creates a `WeightedRandomSampler` for the training set.

        Parameters:
            stage : str, optional
                The current stage ('fit', 'validate', 'test', 'predict').
                This parameter is typically ignored as `setup` prepares all splits.
        """
        # Open the Zarr root group in read-only mode.
        root = zarr.open_group(self.path, mode='r')

        # Determine the main variable and its group to extract metadata.
        in_vars = self.var_info.get('in_vars', [])
        nested_output = len(in_vars) > 0 and isinstance(in_vars[0], (list, tuple))
        main_var = in_vars[0][0] if nested_output else (in_vars[0] if in_vars else None)
        main_group = main_var.split('/')[0]

        if main_var is None:
            raise ValueError("No 'in_vars' defined in var_info. Cannot determine main variable for setup.")

        # Convert transform configurations (names and params) into callable functions.
        # This modifies `self.var_info["transforms"]` in-place.
        set_transforms(self.var_info["transforms"])

        # Get temporal metadata from the main Zarr group
        start_time = root[main_group].attrs['start_time']
        end_time = root[main_group].attrs['end_time']
        min_interval = root[main_group].attrs['interval_minutes']
        timestamps = pd.date_range(start_time, end_time, freq=f"{min_interval}min")

        # Define the full kernel size for sample generation: (temporal_length, height, width).
        kernel = (self.context_len+self.forecast_len, ) + self.img_size

        # Load the array used for sample weighting (e.g., max intensity grid).
        sample_array_path = self.var_info.get('sample_var')
        if sample_array_path is None:
            raise ValueError("Missing 'sample_var' in var_info. Cannot generate samples.")
        
        try:
            sample_array = root[sample_array_path][:]
        except KeyError:
            raise KeyError(f"Sample variable '{sample_array_path}' not found in Zarr store.")

        # Create a global time mask to filter out missing data or high clutter periods.
        time_mask = self._create_time_mask(root, len(sample_array))

        # Generate DataFrames for each split (train, val, test) containing sample indices and weights.
        dfs = get_sample_dfs(
            sample_array=sample_array, 
            kernel=kernel,
            stride=self.stride, 
            timestamps=timestamps,
            split_rules=self.split_info['split_rules'],
            methods=self.sample_info.get('methods', {}),
            time_mask=time_mask
        )
        
        # Instantiate NowcastingDataset for each split.
        self.datasets = {}
        for name, df in dfs.items():
            # Apply an optional threshold filter to the sample weights.
            if 'threshold' in self.sample_info:
                initial_len = len(df)
                # The threshold is applied to `df['weight']`. The threshold value is multiplied by (255/50)
                # to reverse the applied scaling, from 0-255 to 0-50 mm-h. 
                df = df[df['weight'] > (self.sample_info['threshold'] * (255/50))] # Adjust scaler if your weight unit is different
                df.reset_index(drop=True, inplace=True) # Reset index after filtering
                print(f"Split '{name}': {initial_len} samples initially, {len(df)} after weight thresholding.")
            
            # Print some info about the created DataFrame (e.g., length, max weight sample index)
            if not df.empty:
                print(f"Split '{name}': Total samples: {len(df)}, Max weight sample index: {df['weight'].idxmax()}")
            else:
                print(f"Split '{name}': No samples found after filtering.")

            self.datasets[name] = NowcastingDataset(
                df=df,
                root=root,
                info=self.var_info,
                size=self.img_size,
                context_len=self.context_len,
                forecast_len=self.forecast_len,
                include_timestamps=self.include_timestamps
            )
            # Create a weighted sampler only for the training dataset if 'bins' are configured.
            if name == 'train' and 'bins' in self.sample_info:
                self._create_sampler(df, self.sample_info['bins'])

    def _create_time_mask(self, root: zarr.Group, num_timesteps: int) -> np.ndarray:
        """
        Generates a global boolean time mask based on missing data and clutter scores.

        This internal helper function combines multiple masking criteria to identify
        valid time steps across the entire dataset.

        Parameters:
            root : zarr.Group
                The root Zarr group of the dataset.
            num_timesteps : int
                The total number of time steps in the global time axis.

        Returns:
            np.ndarray
                A 1D boolean NumPy array of length `num_timesteps`. `True` indicates
                a valid time step, `False` indicates a masked (invalid) time step.
        """
        global_mask = np.ones(num_timesteps, dtype=bool)
        
        # Apply masks for missing data from specified Zarr groups.
        # It iterates through groups listed in `apply_missing_masks` and uses their
        # `time_mask` datasets, adjusting for different temporal intervals.
        for group_name in self.split_info.get('apply_missing_masks', []):
            if group_name not in root:
                warnings.warn(f"Missing data mask requested for group '{group_name}', but it's not found in Zarr root. Skipping.")
                continue

            group = root[group_name]
            # Get the temporal scaling factor for this group relative to the 5-minute global interval.
            factor = group.attrs['interval_minutes'] // 5
            
            if 'time_mask' not in group:
                warnings.warn(f"Group '{group_name}' does not have a 'time_mask' dataset. Skipping its missing data mask.")
                continue

            group_mask = group['time_mask'][:]
            
            # Repeat the group's mask to match the global 5-minute granularity
            # and apply it as a logical AND to the global mask.
            global_mask &= np.repeat(group_mask, factor)

        # Apply mask based on radar clutter score.
        # Samples with a clutter score above `clutter_threshold` are masked out.
        if 'radar' in root and 'clutter_score' in root['radar']:
            clutter = root['radar']['clutter_score'][:]
            # Use `float('inf')` as default for `clutter_threshold` if not specified,
            # effectively disabling this mask unless explicitly set.
            clutter_threshold = self.split_info.get('clutter_threshold', float('inf'))
            clutter_mask = clutter < clutter_threshold
            global_mask &= clutter_mask
        else:
            warnings.warn("Radar group or 'clutter_score' dataset not found. Skipping clutter masking.")

        return global_mask

    def _create_sampler(self, df: pd.DataFrame, cfg: dict[str, object]):
        """
        Creates a `WeightedRandomSampler` for the training data based on sample weights.

        This method is used to implement a form of importance sampling, typically
        to oversample rare but important events (e.g., heavy rainfall). It bins
        the sample 'weight' values and assigns inverse frequency weights.

        Parameters:
            df : pd.DataFrame
                The DataFrame of training samples, containing the 'weight' column.
            cfg : dict
                Configuration dictionary for the sampler, expected to contain:
                - 'first_bin' (float): The lower bound for the first bin.
                - 'last_bin' (float): The upper bound for the last bin.
                - 'n_bins' (int): Number of bins to discretize sample weights.
                - 'slope' (float): Controls the spacing of bins (e.g., for logarithmic spacing).
                - 'scaler' (float): A scaling factor applied to `df['weight']` before binning.
                - 'n_samples' (int, optional): The total number of samples to draw per epoch.
                  Defaults to the length of the DataFrame.
        """
        first_bin = cfg.get('first_bin', 0.2)
        last_bin = cfg.get('last_bin', 12.0)
        n_bins = cfg.get('n_bins', 8)
        slope = cfg.get('slope', 1.0)
        scaler = cfg.get('scaler', 255/50) # Default scaler, specific to radar intensity 0-255 -> 0-50 mm-h
        n_samples = cfg.get('n_samples', len(df))

        # Compute bin edges. The example uses an exponential (logarithmic) spacing
        # for bins, which is common for skewed distributions like rainfall intensity.
        # The `scaler` is applied to `first_bin` and `last_bin` to match the units
        # of the `df['weight']` column.
        edges = np.exp(np.linspace(np.log(first_bin * scaler), 
                                   np.log(last_bin * scaler), 
                                   n_bins - 1))
        
        # Re-calculating edges based on a base_edges and slope, which seems to be a more
        # flexible way to define non-linear bin spacing.
        base_edges = np.linspace(0, 1, n_bins - 1)
        edges = np.exp(np.log(first_bin) + (np.log(last_bin) - np.log(first_bin)) * (base_edges ** slope))

        # Digitize (bin) the 'weight' values from the DataFrame.
        bin_idx = np.digitize(df['weight'].values, edges)

        # Count samples in each bin. `minlength=n_bins` ensures all bins are represented, even if empty.
        counts = np.bincount(bin_idx, minlength=n_bins)

        # Avoid division by zero for empty bins by setting their count to 1.
        counts = np.where(counts == 0, 1, counts)

        # Calculate inverse frequency weights: rarer bins get higher weights.
        weights = 1.0 / counts
        # Normalize weights so the first bin (typically lowest intensity) has a weight of 1.0.
        weights = (weights / weights[0]).astype(np.float16) # normalize weights

        # Assign a weight to each individual sample based on its bin.
        sample_weights = weights[bin_idx]

        self.sampler = WeightedRandomSampler(
            weights=torch.from_numpy(sample_weights),
            num_samples=n_samples,
            replacement=True
        )

    def train_dataloader(self):
        if self.sampler is None:
            return DataLoader(
                self.datasets['train'],
                batch_size=self.batch_size,
                num_workers=self.num_workers,
                shuffle=True,
                # pin_memory=True,
                persistent_workers=self.num_workers > 0,
            )
        return DataLoader(
            self.datasets['train'],
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            sampler=self.sampler,
            pin_memory=True,
            persistent_workers=self.num_workers > 0,
        )

    def val_dataloader(self):
        return DataLoader(
            self.datasets['val'],
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=False,
            pin_memory=True,
            persistent_workers=self.num_workers > 0
        )
    
    def test_dataloader(self):
        return DataLoader(
            self.datasets['test'],
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=False,
            pin_memory=True,
            persistent_workers=self.num_workers > 0
        )

if __name__ == "__main__":
    # This block demonstrates how to instantiate and use the NowcastingDataModule.
    # It also includes a basic timing test and a utility to print the structure
    # of the data batches.

    # --- Configuration Parameters ---
    # Define the path to your Zarr dataset. Replace with your actual path.
    path = '/vol/knmimo-nobackup/users/mrobben/nowcasting/data/dataset_regrid.zarr'

    # var_info: Defines which variables to load, their transformations, and how inputs are structured.
    var_info = {
        "sample_var": "radar/max_intensity_grid", # Variable used to generate sample weights
        "in_vars": [
            # Example of nested 'in_vars': Each sub-list will be treated as a separate input group.
            # This is useful for multi-modal models that expect different data types as separate inputs.
            ["radar/rtcor"], 
            # Uncomment and adjust paths to include other satellite or harmonie data:
            # ["sat_l1p5/WV_062", "sat_l1p5/IR_108"],
            # ["harmonie/PRES_GDS0_GPML", "harmonie/DPT_GDS0_HTGL", "harmonie/R_H_GDS0_HTGL",
            #  "harmonie/A_PCP_GDS0_HTGL_acc", "harmonie/T_CDC_GDS0_HTGL", "harmonie/KNMI_var_201_entatm"],
            # If you prefer a single concatenated input tensor for all inputs, use a flat list:
            # "in_vars": ["radar/rtcor", "sat_l1p5/WV_062", "harmonie/PRES_GDS0_GPML"],
        ],
        "out_vars": [
            "radar/rtcor" # Target variable(s) for the model to predict
        ],
        # latlon: Set to True to include static latitude and longitude maps as additional input channels.
        "latlon": False, 
        "transforms": {
            # Define transformations to apply to specific Zarr groups or variables.
            # These refer to functions defined in `nowcasting.data.utils.transforms`.
            "radar": {
                "dbz_normalization": {"convert_to_dbz": True}, # Convert radar data to dBZ and normalize
            },
        }
    }

    # split_info: Defines how the dataset is split into train/val/test sets based on time.
    split_info = {
        'split_rules': {
            "test": {"year": [2023]}, # Data from 2023 for the test set
            "val":  {"month": [6, 11]}, # Data from June and November of other years for validation
            "train": {} # All remaining data for the training set (acts as a default)
        },
        # apply_missing_masks: List of Zarr groups whose 'time_mask' should be used to filter out missing data.
        'apply_missing_masks': ['radar', 'harmonie', 'sat_l2', 'sat_l1p5', 'aws'],
        # clutter_threshold: Radar clutter score threshold. Samples with clutter score > 50 are ignored.
        'clutter_threshold': 50,
    }   

    # sample_info: Configuration for how samples are generated and optionally weighted.
    sample_info = {
        # threshold: Only include samples where the 'weight' (from 'sample_var') is above this value.
        # The value (25.0) is scaled by (255/50) in the code, so it's effectively 25.0 * (255/50) in raw units.
        'threshold': 25.0, 
        'methods': {
            # Aggregation method for generating sample weights for each split.
            # 'max_pool' means the weight is the maximum value within the sample's window.
            'train': {'agg': 'max_pool'},
            'val': {'agg': 'max_pool', 'center_crop': True}, # 'val' also applies a center crop
            'test': {'agg': 'max_pool'}
        },
        # bins: Configuration for weighted random sampling (uncomment to enable).
        # This helps in balancing datasets with rare events (e.g., heavy rain).
        # 'bins':{
        #     'n_bins': 30,       # Number of bins for sample weights
        #     'first_bin': 0.02,  # Lower bound of the first bin (scaled)
        #     'last_bin': 30,     # Upper bound of the last bin (scaled)
        #     'slope': 0.2,       # Controls the non-linear spacing of bins
        #     'scaler': 5.1,      # Scaling factor for sample weights before binning (e.g., 255/50 for radar)
        #     'n_samples': 30_000,# Number of samples to draw per epoch for training
        # },
    }

    # --- Instantiate and Setup DataModule ---
    print("Initializing NowcastingDataModule...")
    data_module = NowcastingDataModule(
        path=path,
        var_info=var_info,
        sample_info=sample_info,
        split_info=split_info,
        context_len=4,       # 4 time steps for input
        forecast_len=18,     # 18 time steps for output
        include_timestamps=True, # Include relative timestamps in context
        img_size=(8,8),      # Spatial patch size: 8 blocks x 8 blocks
        stride=(1,1,1),      # Sample generation stride (t, h, w)
        batch_size=8,        # Batch size for DataLoaders
        num_workers=8,       # Number of CPU workers for data loading
    )
    print("Setting up DataModule (loading metadata, generating samples, applying transforms)...")
    data_module.setup() # This is a crucial step that prepares the datasets

    # --- Data Inspection and Timing Test ---
    print("\n--- Inspecting First Validation Batch ---")
    import time
    start = time.time()

    def print_structure(x: object, indent: int = 0):
        """
        Recursively prints the structure (shape for tensors, elements for lists/tuples)
        of a data batch. Useful for debugging and understanding complex outputs.
        """
        prefix = " " * indent
        if isinstance(x, torch.Tensor):
            print(f"{prefix}Tensor shape: {tuple(x.shape)}, dtype: {x.dtype}")
        elif isinstance(x, (list, tuple)):
            print(f"{prefix}{type(x).__name__}[")
            for elem in x:
                print_structure(elem, indent + 2)
            print(f"{prefix}]")
        else:
            print(f"{prefix}{type(x)} # not a Tensor or list/tuple")

    # Iterate through the first batch of the validation dataloader to inspect its structure and time loading.
    for i, (context, future) in enumerate(data_module.val_dataloader()):
        print(f"Batch {i}:")
        print("  Context (Input to Model):")
        print_structure(context, indent=4) # Use 4 spaces for better readability
        print("  Future (Target for Model):")
        print_structure(future, indent=4)
        if i == 0: # Only print for the first batch
            elapsed = time.time() - start
            print(f"\nTime to load first validation batch: {elapsed:.3f} seconds")
            break